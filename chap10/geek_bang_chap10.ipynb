{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_tutorial_final.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm9iNMw5nMtl"
      },
      "source": [
        "# PyTorch Training Short:\n",
        "\n",
        "\n",
        "---\n",
        "First let us check whether the whole thing is working\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CI_nJEend03"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change b type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6u2qdWjdct8"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZkTfRt8dt2i"
      },
      "source": [
        "!pip install pytorch-lightning==1.1.0rc1 sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p3Tbx8cWEFA"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0J_DnaeoBTi"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42H2EckBoKAf"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5MT1Sh1ot60"
      },
      "source": [
        "### Create a simple dataset to test the correctness of our approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vPVo_jIoZ9Y"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "x = torch.randn(100000, 2)\n",
        "noise = torch.randn(100000,)\n",
        "y = ((1.0*x[:,0]+2.0*x[:,1]+noise)>0).type(torch.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz_JAic0o1_L"
      },
      "source": [
        "y_np = y.numpy()\n",
        "x_np = x.numpy()\n",
        "y_train, y_test = y_np[:50000], y_np[50000:]\n",
        "x_train, x_test = x_np[:50000, :], x_np[50000:, :]\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(x_train, y_train)\n",
        "y_pred = log_reg.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjdyrSD2pIoX"
      },
      "source": [
        "### Now create an evil data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuI0QJF-o_fe"
      },
      "source": [
        "x_1 = torch.randn(100000)\n",
        "x_2 = torch.randn(100000)\n",
        "x_useful = torch.cos(1.5*x_1)*(x_2**2)\n",
        "x_1_rest_small = torch.randn(100000, 15)+ 0.01*x_1.unsqueeze(1)\n",
        "x_1_rest_large = torch.randn(100000, 15) + 0.1*x_1.unsqueeze(1)\n",
        "x_2_rest_small = torch.randn(100000, 15)+ 0.01*x_2.unsqueeze(1)\n",
        "x_2_rest_large = torch.randn(100000, 15) + 0.1*x_2.unsqueeze(1)\n",
        "x = torch.cat([x_1[:, None], x_2[:, None], x_1_rest_small, x_1_rest_large, x_2_rest_small, x_2_rest_large], dim=1)\n",
        "y = ((10*x_useful) + 5*torch.randn(100000) >0.0).type(torch.int64) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zBxF0QCp-Na"
      },
      "source": [
        "### Now let us test if we have an oracle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3jSEESWp6mN"
      },
      "source": [
        "y_train, y_test = y.numpy()[:50000], y.numpy()[50000:]\n",
        "x_train, x_test = x.numpy()[:50000, :], x.numpy()[50000:, :]\n",
        "oracle_train, oracle_test = x_useful.numpy()[:50000], x_useful.numpy()[50000:]\n",
        "log_reg_2 = LogisticRegression()\n",
        "log_reg_2.fit(oracle_train[:, None],y_train)\n",
        "y_pred = log_reg_2.predict(oracle_test[:, None])\n",
        "print(accuracy_score(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUZ94ad7qVhZ"
      },
      "source": [
        "### What if the oracle is not here?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SETnkiN1qTHU"
      },
      "source": [
        "y_train, y_test = y.numpy()[:50000], y.numpy()[50000:]\n",
        "x_train, x_test = x.numpy()[:50000, :], x.numpy()[50000:, :]\n",
        "log_reg_3 = LogisticRegression()\n",
        "log_reg_3.fit(x_train, y_train)\n",
        "y_pred = log_reg_3.predict(x_test)\n",
        "accuracy_score(y_pred, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH7Anlg0qg2j"
      },
      "source": [
        "### Let us run a basic PyTorch example to test whether the good is correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXXQ_Tbzqe8k"
      },
      "source": [
        "x = torch.randn(100000, 2)\n",
        "noise = torch.randn(100000,)\n",
        "y = ((1.0*x[:,0]+2.0*x[:,1]+noise)>0).type(torch.int64)\n",
        "x_train, x_test = x[:50000, :], x[50000:, :]\n",
        "y_train, y_test = y[:50000], y[50000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0R12sOnquZq"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MyDataSet(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        super().__init__()\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.len = x.shape[0]\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx, :], self.y[idx]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnHG9ujxqxZt"
      },
      "source": [
        "train_dataset = MyDataSet(x_train, y_train)\n",
        "test_dataset = MyDataSet(x_test, y_test)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 128, shuffle=True, num_workers=6)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 128, num_workers=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO7lkElKrEOO"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def mish(input):\n",
        "\n",
        "    return input * torch.tanh(F.softplus(input))\n",
        "\n",
        "class Mish(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Init method.\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        Forward pass of the function.\n",
        "        '''\n",
        "        return mish(input)\n",
        "\n",
        "class MLPLayer(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, res_coef = 0, dropout_p = 0.1):\n",
        "        super().__init__()\n",
        "        self.linear  = nn.Linear(dim_in, dim_out)\n",
        "        self.res_coef = res_coef\n",
        "        self.activation = Mish()\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.ln = nn.LayerNorm(dim_out)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        y = self.linear(x)\n",
        "        y = self.activation(y)\n",
        "        y = self.dropout(y)\n",
        "        if self.res_coef == 0:\n",
        "            return self.ln(y)\n",
        "        else:\n",
        "            return self.ln(self.res_coef*x +y )\n",
        "\n",
        "       \n",
        "class MyNetwork(nn.Module):\n",
        "    def __init__(self, dim_in, dim, res_coef=0.5, dropout_p = 0.1, n_layers = 10):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.ModuleList()\n",
        "        self.first_linear = MLPLayer(dim_in, dim)\n",
        "        self.n_layers = n_layers\n",
        "        for i in range(n_layers):\n",
        "            self.mlp.append(MLPLayer(dim, dim, res_coef, dropout_p))\n",
        "        self.final = nn.Linear(dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.first_linear(x)\n",
        "        for layer in self.mlp:\n",
        "            x = layer(x)\n",
        "        x= self.sigmoid(self.final(x))\n",
        "        return x.squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ka-3_rfrTKw"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.metrics import Accuracy\n",
        "class TrainingModule(pl.LightningModule):\n",
        "    def __init__(self, dim_in, dim, res_coef=0, dropout_p=0, n_layers=10):\n",
        "        super().__init__()\n",
        "        self.backbone = MyNetwork(dim_in, dim, res_coef, dropout_p, n_layers)\n",
        "        self.loss = nn.BCELoss()\n",
        "        self.accuracy = Accuracy()\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = self.backbone(x)\n",
        "        loss = self.loss(x, y.type(torch.float32))\n",
        "        acc = self.accuracy(x, y)\n",
        "        self.log(\"Validation loss\", loss)\n",
        "        self.log(\"Validation acc\", acc)\n",
        "        return loss, acc\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = self.backbone(x)\n",
        "        loss = self.loss(x, y.type(torch.float32))\n",
        "        acc = self.accuracy(x, y)\n",
        "        self.log(\"Training loss\", loss)\n",
        "        self.log(\"Training acc\", acc)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "import os\n",
        "class CheckpointEveryNSteps(pl.Callback):\n",
        "    def __init__(self, save_step_frequency):\n",
        "        self.save_step_frequency = save_step_frequency\n",
        "\n",
        "    def on_batch_end(self, trainer: pl.Trainer, _):\n",
        "        epoch = trainer.current_epoch\n",
        "        global_step = trainer.global_step\n",
        "        if global_step % self.save_step_frequency == 0:\n",
        "            filename = \"epoch=\" + str(epoch) + \"_step=\" + str(global_step)+\".ckpt\"\n",
        "            ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
        "            trainer.save_checkpoint(ckpt_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47ns8nDyrd3v"
      },
      "source": [
        "# from pytorch_lightning import loggers as pl_loggers\n",
        "\n",
        "# tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
        "# save_by_steps = CheckpointEveryNSteps(100)\n",
        "# training_module = TrainingModule(2, 10, 0.5, 0.1, 2)\n",
        "# trainer = pl.Trainer(max_epochs=2, gpus=1, progress_bar_refresh_rate=100, val_check_interval=0.25, logger=tb_logger)\n",
        "# trainer.fit(training_module, train_dataloader, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9851mNJrlFi"
      },
      "source": [
        "# %tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru23N23JSCNM"
      },
      "source": [
        "\n",
        "### Now Let us use the evil dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQRdDlCTSh9T"
      },
      "source": [
        "import torch\n",
        "x_1 = torch.randn(100000)\n",
        "x_2 = torch.randn(100000)\n",
        "x_useful = torch.cos(1.5*x_1)*(x_2**2)\n",
        "x_1_rest_small = torch.randn(100000, 15)+ 0.01*x_1.unsqueeze(1)\n",
        "x_1_rest_large = torch.randn(100000, 15) + 0.1*x_1.unsqueeze(1)\n",
        "x_2_rest_small = torch.randn(100000, 15)+ 0.01*x_2.unsqueeze(1)\n",
        "x_2_rest_large = torch.randn(100000, 15) + 0.1*x_2.unsqueeze(1)\n",
        "x = torch.cat([x_1[:, None], x_2[:, None], x_1_rest_small, x_1_rest_large, x_2_rest_small, x_2_rest_large], dim=1)\n",
        "y = ((10*x_useful) + 5*torch.randn(100000) >0.0).type(torch.int64) \n",
        "\n",
        "x_train, x_test = x[:50000, :], x[50000:, :]\n",
        "y_train, y_test = y[:50000], y[50000:]\n",
        "train_dataset = MyDataSet(x_train, y_train)\n",
        "test_dataset = MyDataSet(x_test, y_test)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 32, num_workers=6)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 128, num_workers=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSR6VqoaS-L9"
      },
      "source": [
        "# from pytorch_lightning import loggers as pl_loggers\n",
        "\n",
        "# tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
        "\n",
        "# training_module = TrainingModule(62, 32, 0.5, 0.1, 20)\n",
        "# trainer = pl.Trainer(max_epochs=20, tpu_cores=8, progress_bar_refresh_rate=100, val_check_interval=0.5, logger=tb_logger)\n",
        "# trainer.fit(training_module, train_dataloader, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dGk-DQETEs0"
      },
      "source": [
        "# %tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjdSSleYTN3o"
      },
      "source": [
        "Now let us see how LightGBM works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF66ZMrMTS6X"
      },
      "source": [
        "# import lightgbm as lgb\n",
        "\n",
        "# x_train_np, x_test_np = x_train.numpy(), x_test.numpy()\n",
        "# y_train_np, y_test_np = y_train.numpy(), y_test.numpy()\n",
        "\n",
        "# train_dataset = lgb.Dataset(x_train_np, y_train_np)\n",
        "# test_dataset = lgb.Dataset(x_test_np, y_test_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-pESKZ2UAXC"
      },
      "source": [
        "# params = {'num_leaves': 31, 'objective': 'binary', 'feature_fraction':0.8, 'bagging_fraction':0.8, 'metric':'binary_error'}\n",
        "# num_round=2000\n",
        "# eval_list = [train_dataset, test_dataset]\n",
        "# lgb_model = lgb.train(params, train_dataset, num_round, valid_sets=eval_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Orx1fAGVJGj"
      },
      "source": [
        "How can we improve (save) our deep learning model. \n",
        "1. Discretize\n",
        "2. Variable selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSeOMEFlVjUN"
      },
      "source": [
        "# coding = 'utf-8'\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "\n",
        "def encode_label(x):\n",
        "    unique=sorted(list(set([str(item) for item in np.unique(x)])))\n",
        "    kv = {unique[i]: i for i in range(len(unique))}\n",
        "    vfunc = np.vectorize(lambda x: kv[str(x)])\n",
        "    return vfunc(x)\n",
        "\n",
        "def encode_label_mat(x):\n",
        "    _, ncol = x.shape\n",
        "    result = np.empty_like(x, dtype=int)\n",
        "    for col in range(ncol):\n",
        "        result[:,col] = encode_label(x[:, col])\n",
        "    return result\n",
        "\n",
        "def impute_nan(x, method='median'):\n",
        "    _, ncol = x.shape\n",
        "    result = np.empty_like(x)\n",
        "\n",
        "    for col in range(ncol):\n",
        "        if method == 'median':\n",
        "            data = x[:, col]\n",
        "            impute_value = np.median(data[~pd.isnull(data) & (data != np.inf) & (data != -np.inf)])\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        func = np.vectorize(lambda x: impute_value if pd.isnull(x) else x)\n",
        "        result[:, col] = func(x[:, col])\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_uniform_interval(minimum, maximum, nbins):\n",
        "    result = [minimum]\n",
        "    step_size = (float(maximum - minimum)) / nbins\n",
        "    for index in range(nbins - 1):\n",
        "        result.append(minimum + step_size * (index + 1))\n",
        "    result.append(maximum)\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_interval_v2(x, sorted_intervals):\n",
        "    if pd.isnull(x):\n",
        "        return -1\n",
        "    if x == np.inf:\n",
        "        return -2\n",
        "    if x == -np.inf:\n",
        "        return -3\n",
        "    interval = 0\n",
        "    found = False\n",
        "    sorted_intervals.append(np.inf)\n",
        "    while not found and interval < len(sorted_intervals) - 1:\n",
        "        if sorted_intervals[interval] <= x < sorted_intervals[interval + 1]:\n",
        "            return interval\n",
        "        else:\n",
        "            interval += 1\n",
        "\n",
        "\n",
        "def get_quantile_interval(data, nbins):\n",
        "    quantiles = get_uniform_interval(0, 1, nbins)\n",
        "    return list(np.quantile(data[(~pd.isnull(data)) & (data != np.inf) & (data != -np.inf)], quantiles))\n",
        "\n",
        "\n",
        "def discretize(x, nbins=20):\n",
        "    nrow, ncol = x.shape\n",
        "    result = np.empty_like(x)\n",
        "    interval_list = list()\n",
        "    for col in range(ncol):\n",
        "        intervals = sorted(list(set(get_quantile_interval(x[:, col], nbins))))\n",
        "        interval_centroid = list()\n",
        "\n",
        "        for i in range(len(intervals) - 1):\n",
        "            interval_centroid.append(0.5 * (intervals[i] + intervals[i + 1]))\n",
        "        func = np.vectorize(lambda x: get_interval_v2(x, intervals))\n",
        "        result[:, col] = encode_label(func(x[:, col]))\n",
        "        interval_list.append(interval_centroid)\n",
        "    return result.astype(np.int64), interval_list\n",
        "\n",
        "def get_var_type(df):\n",
        "    columns = df.columns\n",
        "    continuous_vars = [x for x in columns if x.startswith('continuous_')]\n",
        "    discrete_vars = [x for x in columns if x.startswith('discrete_')]\n",
        "    other_vars = list()\n",
        "    for column in columns:\n",
        "        if column not in continuous_vars and column not in discrete_vars:\n",
        "            other_vars.append(column)\n",
        "    return {'continuous': continuous_vars,\n",
        "            'discrete': discrete_vars,\n",
        "            'other': other_vars}\n",
        "\n",
        "\n",
        "def get_cont_var(df):\n",
        "    var_types = get_var_type(df)\n",
        "    return var_types['continuous']\n",
        "\n",
        "\n",
        "def get_dis_var(df):\n",
        "    var_types = get_var_type(df)\n",
        "    return var_types['discrete']\n",
        "\n",
        "def drop_const_var(data):\n",
        "    result = data.copy(deep=True)\n",
        "    for col in data.columns:\n",
        "        if len(data.loc[~pd.isnull(data[col]), col].unique()) <= 1:\n",
        "            result.drop(columns=col, inplace=True)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-nHUa7FYU8G"
      },
      "source": [
        "x_train_np, x_test_np = x_train.numpy(), x_test.numpy()\n",
        "y_train_np, y_test_np = y_train.numpy(), y_test.numpy()\n",
        "x = np.concatenate([x_train_np, x_test_np])\n",
        "x_dis, centroids = discretize(x)\n",
        "x_dis_train = x_dis[:50000, :]\n",
        "x_dis_test = x_dis[50000:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WViXd9Ldc_Uy"
      },
      "source": [
        "class TabDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        super().__init__()\n",
        "        self.x = torch.from_numpy(x).type(torch.int64) \n",
        "        self.y = torch.from_numpy(y).type(torch.float32).squeeze() \n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx, :], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWljva98c_Uy"
      },
      "source": [
        "### Create Embedding Factories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVX2rMNMc_Uy"
      },
      "source": [
        "!pip install einops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cXlyVDEchPe"
      },
      "source": [
        "class EmbeddingFactory(nn.Module):\n",
        "    def __init__(self, x, dim_out):\n",
        "        super().__init__()\n",
        "        self.dim_out = dim_out\n",
        "        self.module_list = nn.ModuleList(\n",
        "            [nn.Embedding(len(set(np.unique(x[:, col]))), dim_out) for col in range(x.shape[1])])\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = [self.module_list[col](x[:, col]).unsqueeze(2) for col in range(x.shape[1])]\n",
        "        return torch.cat(result, dim=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_LgwyAjc_Uz"
      },
      "source": [
        "from einops import rearrange, reduce, repeat\n",
        "x_dis_test.shape\n",
        "train_dataloader = DataLoader(TabDataset(x_dis_train, y_train_np), batch_size = 32, num_workers=6)\n",
        "test_dataloader = DataLoader(TabDataset(x_dis_test, y_test_np), batch_size = 128, num_workers=6)\n",
        "\n",
        "class TrainingModuleV2(pl.LightningModule):\n",
        "    def __init__(self, x, dim_emb, dim_mlp, res_coef=0, dropout_p=0, n_layers=10):\n",
        "        super().__init__()\n",
        "        self.embedding = EmbeddingFactory(x, dim_emb)\n",
        "        self.backbone = MyNetwork(x.shape[1]*dim_emb, dim_mlp, res_coef, dropout_p, n_layers)\n",
        "        self.loss = nn.BCELoss()\n",
        "        self.accuracy = Accuracy()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        return self.backbone(x)\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = self.embedding(x)\n",
        "        x = rearrange(x, \"b h e -> b (h e)\")\n",
        "        x = self.backbone(x)\n",
        "        loss = self.loss(x, y)\n",
        "        acc = self.accuracy(x, y)\n",
        "        self.log(\"Validation loss\", loss)\n",
        "        self.log(\"Validation acc\", acc)\n",
        "        return loss, acc\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = self.embedding(x)\n",
        "        x = rearrange(x, \"b h e -> b (h e)\")\n",
        "        x = self.backbone(x)\n",
        "        loss = self.loss(x, y)\n",
        "        acc = self.accuracy(x, y)\n",
        "        self.log(\"Training loss\", loss)\n",
        "        self.log(\"Training acc\", acc)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpCnDu0Hc_Uz"
      },
      "source": [
        "# from pytorch_lightning import loggers as pl_loggers\n",
        "\n",
        "# tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
        "# training_module = TrainingModuleV2(x_dis, 16, 64, 0.5, 0.1, 10)\n",
        "# trainer = pl.Trainer(max_epochs=10, gpus=1, progress_bar_refresh_rate=100, val_check_interval=0.5, logger=tb_logger)\n",
        "# trainer.fit(training_module, train_dataloader, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y66ZQw5Ec_Uz"
      },
      "source": [
        "# %tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfBt32oNc_U0"
      },
      "source": [
        "### Let us try TabNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN1dxePIoqCY"
      },
      "source": [
        "!pip install einops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XIjemhTpOvz"
      },
      "source": [
        "import torch\n",
        "x_1 = torch.randn(100000)\n",
        "x_2 = torch.randn(100000)\n",
        "x_useful = torch.cos(1.5*x_1)*(x_2**2)\n",
        "x_1_rest_small = torch.randn(100000, 15)+ 0.01*x_1.unsqueeze(1)\n",
        "x_1_rest_large = torch.randn(100000, 15) + 0.1*x_1.unsqueeze(1)\n",
        "x_2_rest_small = torch.randn(100000, 15)+ 0.01*x_2.unsqueeze(1)\n",
        "x_2_rest_large = torch.randn(100000, 15) + 0.1*x_2.unsqueeze(1)\n",
        "x = torch.cat([x_1[:, None], x_2[:, None], x_1_rest_small, x_1_rest_large, x_2_rest_small, x_2_rest_large], dim=1)\n",
        "y = ((10*x_useful) + 5*torch.randn(100000) >0.0).type(torch.int64) \n",
        "\n",
        "x_train, x_test = x[:50000, :], x[50000:, :]\n",
        "y_train, y_test = y[:50000], y[50000:]\n",
        "x_train_np, x_test_np = x_train.numpy(), x_test.numpy()\n",
        "y_train_np, y_test_np = y_train.numpy(), y_test.numpy()\n",
        "x = np.concatenate([x_train_np, x_test_np])\n",
        "x_dis, centroids = discretize(x)\n",
        "x_dis_train = x_dis[:50000, :]\n",
        "x_dis_test = x_dis[50000:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY2Lbb-Fc_U1"
      },
      "source": [
        "from torch import nn\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "\n",
        "\"\"\"\n",
        "Other possible implementations:\n",
        "https://github.com/KrisKorrel/sparsemax-pytorch/blob/master/sparsemax.py\n",
        "https://github.com/msobroza/SparsemaxPytorch/blob/master/mnist/sparsemax.py\n",
        "https://github.com/vene/sparse-structured-attention/blob/master/pytorch/torchsparseattn/sparsemax.py\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# credits to Yandex https://github.com/Qwicen/node/blob/master/lib/nn_utils.py\n",
        "def _make_ix_like(input, dim=0):\n",
        "    d = input.size(dim)\n",
        "    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)\n",
        "    view = [1] * input.dim()\n",
        "    view[0] = -1\n",
        "    return rho.view(view).transpose(0, dim)\n",
        "\n",
        "\n",
        "class SparsemaxFunction(Function):\n",
        "    \"\"\"\n",
        "    An implementation of sparsemax (Martins & Astudillo, 2016). See\n",
        "    :cite:`DBLP:journals/corr/MartinsA16` for detailed description.\n",
        "    By Ben Peters and Vlad Niculae\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, dim=-1):\n",
        "        \"\"\"sparsemax: normalizing sparse transform (a la softmax)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ctx : torch.autograd.function._ContextMethodMixin\n",
        "        input : torch.Tensor\n",
        "            any shape\n",
        "        dim : int\n",
        "            dimension along which to apply sparsemax\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        output : torch.Tensor\n",
        "            same shape as input\n",
        "\n",
        "        \"\"\"\n",
        "        ctx.dim = dim\n",
        "        max_val, _ = input.max(dim=dim, keepdim=True)\n",
        "        input -= max_val  # same numerical stability trick as for softmax\n",
        "        tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)\n",
        "        output = torch.clamp(input - tau, min=0)\n",
        "        ctx.save_for_backward(supp_size, output)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        supp_size, output = ctx.saved_tensors\n",
        "        dim = ctx.dim\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input[output == 0] = 0\n",
        "\n",
        "        v_hat = (grad_input.sum(dim=dim) / supp_size).squeeze()\n",
        "        v_hat = v_hat.unsqueeze(dim)\n",
        "        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n",
        "        return grad_input, None\n",
        "\n",
        "    @staticmethod\n",
        "    def _threshold_and_support(input, dim=-1):\n",
        "        \"\"\"Sparsemax building block: compute the threshold\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input: torch.Tensor\n",
        "            any dimension\n",
        "        dim : int\n",
        "            dimension along which to apply the sparsemax\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tau : torch.Tensor\n",
        "            the threshold value\n",
        "        support_size : torch.Tensor\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        input_srt, _ = torch.sort(input, descending=True, dim=dim)\n",
        "        input_cumsum = input_srt.cumsum(dim) - 1\n",
        "        rhos = _make_ix_like(input, dim)\n",
        "        support = rhos * input_srt > input_cumsum\n",
        "\n",
        "        support_size = support.sum(dim=dim).unsqueeze(dim)\n",
        "        tau = input_cumsum.gather(dim, support_size - 1)\n",
        "        tau /= support_size.to(input.dtype)\n",
        "        return tau, support_size\n",
        "\n",
        "\n",
        "sparsemax = SparsemaxFunction.apply\n",
        "\n",
        "\n",
        "class Sparsemax(nn.Module):\n",
        "\n",
        "    def __init__(self, dim=-1):\n",
        "        self.dim = dim\n",
        "        super(Sparsemax, self).__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return sparsemax(input, self.dim)\n",
        "\n",
        "\n",
        "class Entmax15(nn.Module):\n",
        "    def __init__(self, dim=-1):\n",
        "        super().__init_()\n",
        "        self.dim=dim\n",
        "            \n",
        "    @staticmethod\n",
        "    def _threshold_and_support(input, dim=-1):\n",
        "        Xsrt, _ = torch.sort(input, descending=True, dim=dim)\n",
        "\n",
        "        rho = _make_ix_like(input, dim)\n",
        "        mean = Xsrt.cumsum(dim) / rho\n",
        "        mean_sq = (Xsrt ** 2).cumsum(dim) / rho\n",
        "        ss = rho * (mean_sq - mean ** 2)\n",
        "        delta = (1 - ss) / rho\n",
        "\n",
        "        delta_nz = torch.clamp(delta, 0)\n",
        "        tau = mean - torch.sqrt(delta_nz)\n",
        "\n",
        "        support_size = (tau <= Xsrt).sum(dim).unsqueeze(dim)\n",
        "        tau_star = tau.gather(dim, support_size - 1)\n",
        "        return tau_star, support_size\n",
        "    def forward(self, input):\n",
        "        max_val, _ = input.max(dim=self.dim, keepdim=True)\n",
        "        input = input - max_val  # same numerical stability trick as for softmax\n",
        "        input = input / 2  # divide by 2 to solve actual Entmax\n",
        "\n",
        "        tau_star, _ = Entmax15Function._threshold_and_support(input, self.dim)\n",
        "        output = torch.clamp(input - tau_star, min=0) ** 2\n",
        "        ctx.save_for_backward(output)\n",
        "        return output \n",
        "\n",
        "    def backward(self, output, grad):\n",
        "        Y = output\n",
        "        gppr = Y.sqrt()  # = 1 / g'' (Y)\n",
        "        dX = grad_output * gppr\n",
        "        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\n",
        "        q = q.unsqueeze(ctx.dim)\n",
        "        dX -= q * gppr\n",
        "        return dX, None\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDCekrVoc_U1"
      },
      "source": [
        "import torch\n",
        "from torch.nn import Linear, BatchNorm1d, ReLU\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "def initialize_non_glu(module, input_dim, output_dim):\n",
        "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(4*input_dim))\n",
        "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
        "    # torch.nn.init.zeros_(module.bias)\n",
        "    return\n",
        "\n",
        "\n",
        "def initialize_glu(module, input_dim, output_dim):\n",
        "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(input_dim))\n",
        "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
        "    # torch.nn.init.zeros_(module.bias)\n",
        "    return\n",
        "\n",
        "\n",
        "class GBN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "        Ghost Batch Normalization\n",
        "        https://arxiv.org/abs/1705.08741\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, virtual_batch_size=128, momentum=0.01):\n",
        "        super(GBN, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.bn = BatchNorm1d(self.input_dim, momentum=momentum)\n",
        "\n",
        "    def forward(self, x):\n",
        "        chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n",
        "        res = [self.bn(x_) for x_ in chunks]\n",
        "\n",
        "        return torch.cat(res, dim=0)\n",
        "\n",
        "\n",
        "class TabNet(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim,\n",
        "                 n_d=64, n_a=64,\n",
        "                 n_steps=5, gamma=1.3,\n",
        "                 n_independent=2, n_shared=2, epsilon=1e-15,\n",
        "                 virtual_batch_size=128, momentum=0.02,\n",
        "                 mask_type=\"sparsemax\"):\n",
        "        \"\"\"\n",
        "        Defines main part of the TabNet network without the embedding layers.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim : int\n",
        "            Number of features\n",
        "        output_dim : int or list of int for multi task classification\n",
        "            Dimension of network output\n",
        "            examples : one for regression, 2 for binary classification etc...\n",
        "        n_d : int\n",
        "            Dimension of the prediction  layer (usually between 4 and 64)\n",
        "        n_a : int\n",
        "            Dimension of the attention  layer (usually between 4 and 64)\n",
        "        n_steps : int\n",
        "            Number of sucessive steps in the newtork (usually betwenn 3 and 10)\n",
        "        gamma : float\n",
        "            Float above 1, scaling factor for attention updates (usually betwenn 1.0 to 2.0)\n",
        "        n_independent : int\n",
        "            Number of independent GLU layer in each GLU block (default 2)\n",
        "        n_shared : int\n",
        "            Number of independent GLU layer in each GLU block (default 2)\n",
        "        epsilon : float\n",
        "            Avoid log(0), this should be kept very low\n",
        "        virtual_batch_size : int\n",
        "            Batch size for Ghost Batch Normalization\n",
        "        momentum : float\n",
        "            Float value between 0 and 1 which will be used for momentum in all batch norm\n",
        "        mask_type : str\n",
        "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
        "        \"\"\"\n",
        "        super(TabNet, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.is_multi_task = isinstance(output_dim, list)\n",
        "        self.n_d = n_d\n",
        "        self.n_a = n_a\n",
        "        self.n_steps = n_steps\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.n_independent = n_independent\n",
        "        self.n_shared = n_shared\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.mask_type = mask_type\n",
        "        self.initial_bn = BatchNorm1d(self.input_dim, momentum=0.01)\n",
        "\n",
        "        if self.n_shared > 0:\n",
        "            shared_feat_transform = torch.nn.ModuleList()\n",
        "            for i in range(self.n_shared):\n",
        "                if i == 0:\n",
        "                    shared_feat_transform.append(Linear(self.input_dim,\n",
        "                                                        2*(n_d + n_a),\n",
        "                                                        bias=False))\n",
        "                else:\n",
        "                    shared_feat_transform.append(Linear(n_d + n_a, 2*(n_d + n_a), bias=False))\n",
        "\n",
        "        else:\n",
        "            shared_feat_transform = None\n",
        "\n",
        "        self.initial_splitter = FeatTransformer(self.input_dim, n_d+n_a, shared_feat_transform,\n",
        "                                                n_glu_independent=self.n_independent,\n",
        "                                                virtual_batch_size=self.virtual_batch_size,\n",
        "                                                momentum=momentum)\n",
        "\n",
        "        self.feat_transformers = torch.nn.ModuleList()\n",
        "        self.att_transformers = torch.nn.ModuleList()\n",
        "\n",
        "        for step in range(n_steps):\n",
        "            transformer = FeatTransformer(self.input_dim, n_d+n_a, shared_feat_transform,\n",
        "                                          n_glu_independent=self.n_independent,\n",
        "                                          virtual_batch_size=self.virtual_batch_size,\n",
        "                                          momentum=momentum)\n",
        "            attention = AttentiveTransformer(n_a, self.input_dim,\n",
        "                                             virtual_batch_size=self.virtual_batch_size,\n",
        "                                             momentum=momentum,\n",
        "                                             mask_type=self.mask_type)\n",
        "            self.feat_transformers.append(transformer)\n",
        "            self.att_transformers.append(attention)\n",
        "\n",
        "        if self.is_multi_task:\n",
        "            self.multi_task_mappings = torch.nn.ModuleList()\n",
        "            for task_dim in output_dim:\n",
        "                task_mapping = Linear(n_d, task_dim, bias=False)\n",
        "                initialize_non_glu(task_mapping, n_d, task_dim)\n",
        "                self.multi_task_mappings.append(task_mapping)\n",
        "        else:\n",
        "            self.final_mapping = Linear(n_d, output_dim, bias=False)\n",
        "            initialize_non_glu(self.final_mapping, n_d, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = 0\n",
        "        x = self.initial_bn(x)\n",
        "\n",
        "        prior = torch.ones(x.shape, device=x.device)\n",
        "        M_loss = 0\n",
        "        att = self.initial_splitter(x)[:, self.n_d:]\n",
        "\n",
        "        for step in range(self.n_steps):\n",
        "            M = self.att_transformers[step](prior, att)\n",
        "            M_loss += torch.mean(torch.sum(torch.mul(M, torch.log(M+self.epsilon)),\n",
        "                                           dim=1))\n",
        "            # update prior\n",
        "            prior = torch.mul(self.gamma - M, prior)\n",
        "            # output\n",
        "            masked_x = torch.mul(M, x)\n",
        "            out = self.feat_transformers[step](masked_x)\n",
        "            d = ReLU()(out[:, :self.n_d])\n",
        "            res = torch.add(res, d)\n",
        "            # update attention\n",
        "            att = out[:, self.n_d:]\n",
        "\n",
        "        M_loss /= self.n_steps\n",
        "\n",
        "        if self.is_multi_task:\n",
        "            # Result will be in list format\n",
        "            out = []\n",
        "            for task_mapping in self.multi_task_mappings:\n",
        "                out.append(task_mapping(res))\n",
        "        else:\n",
        "            out = self.final_mapping(res)\n",
        "        return out, M_loss\n",
        "\n",
        "    def forward_masks(self, x):\n",
        "        x = self.initial_bn(x)\n",
        "\n",
        "        prior = torch.ones(x.shape, device=x.device)\n",
        "        M_explain = torch.zeros(x.shape, device=x.device)\n",
        "        att = self.initial_splitter(x)[:, self.n_d:]\n",
        "        masks = {}\n",
        "\n",
        "        for step in range(self.n_steps):\n",
        "            M = self.att_transformers[step](prior, att)\n",
        "            masks[step] = M\n",
        "            # update prior\n",
        "            prior = torch.mul(self.gamma - M, prior)\n",
        "            # output\n",
        "            masked_x = torch.mul(M, x)\n",
        "            out = self.feat_transformers[step](masked_x)\n",
        "            d = ReLU()(out[:, :self.n_d])\n",
        "            # explain\n",
        "            step_importance = torch.sum(d, dim=1)\n",
        "            M_explain += torch.mul(M, step_importance.unsqueeze(dim=1))\n",
        "            # update attention\n",
        "            att = out[:, self.n_d:]\n",
        "\n",
        "        return M_explain, masks\n",
        "\n",
        "class AttentiveTransformer(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim,\n",
        "                 virtual_batch_size=128,\n",
        "                 momentum=0.02,\n",
        "                 mask_type=\"entmax\"):\n",
        "        \"\"\"\n",
        "        Initialize an attention transformer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim : int\n",
        "            Input size\n",
        "        output_dim : int\n",
        "            Outpu_size\n",
        "        virtual_batch_size : int\n",
        "            Batch size for Ghost Batch Normalization\n",
        "        momentum : float\n",
        "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
        "        mask_type : str\n",
        "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
        "        \"\"\"\n",
        "        super(AttentiveTransformer, self).__init__()\n",
        "        self.fc = Linear(input_dim, output_dim, bias=False)\n",
        "        initialize_non_glu(self.fc, input_dim, output_dim)\n",
        "        self.bn = GBN(output_dim, virtual_batch_size=virtual_batch_size,\n",
        "                      momentum=momentum)\n",
        "\n",
        "        if mask_type == \"sparsemax\":\n",
        "            # Sparsemax\n",
        "            self.selector = Sparsemax(dim=-1)\n",
        "        elif mask_type == \"entmax\":\n",
        "            # Entmax\n",
        "            self.selector = Entmax15(dim=-1)\n",
        "        else:\n",
        "            raise NotImplementedError(\"Please choose either sparsemax\" +\n",
        "                                      \"or entmax as masktype\")\n",
        "\n",
        "    def forward(self, priors, processed_feat):\n",
        "        x = self.fc(processed_feat)\n",
        "        x = self.bn(x)\n",
        "        x = torch.mul(x, priors)\n",
        "        x = self.selector(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeatTransformer(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, shared_layers, n_glu_independent,\n",
        "                 virtual_batch_size=128, momentum=0.02):\n",
        "        super(FeatTransformer, self).__init__()\n",
        "        \"\"\"\n",
        "        Initialize a feature transformer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim : int\n",
        "            Input size\n",
        "        output_dim : int\n",
        "            Outpu_size\n",
        "        shared_layers : torch.nn.ModuleList\n",
        "            The shared block that should be common to every step\n",
        "        n_glu_independant : int\n",
        "            Number of independent GLU layers\n",
        "        virtual_batch_size : int\n",
        "            Batch size for Ghost Batch Normalization within GLU block(s)\n",
        "        momentum : float\n",
        "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
        "        \"\"\"\n",
        "\n",
        "        params = {\n",
        "            'n_glu': n_glu_independent,\n",
        "            'virtual_batch_size': virtual_batch_size,\n",
        "            'momentum': momentum\n",
        "        }\n",
        "\n",
        "        if shared_layers is None:\n",
        "            # no shared layers\n",
        "            self.shared = torch.nn.Identity()\n",
        "            is_first = True\n",
        "        else:\n",
        "            self.shared = GLU_Block(input_dim, output_dim,\n",
        "                                    first=True,\n",
        "                                    shared_layers=shared_layers,\n",
        "                                    n_glu=len(shared_layers),\n",
        "                                    virtual_batch_size=virtual_batch_size,\n",
        "                                    momentum=momentum)\n",
        "            is_first = False\n",
        "\n",
        "        if n_glu_independent == 0:\n",
        "            # no independent layers\n",
        "            self.specifics = torch.nn.Identity()\n",
        "        else:\n",
        "            spec_input_dim = input_dim if is_first else output_dim\n",
        "            self.specifics = GLU_Block(spec_input_dim, output_dim,\n",
        "                                       first=is_first,\n",
        "                                       **params)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.shared(x)\n",
        "        x = self.specifics(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GLU_Block(torch.nn.Module):\n",
        "    \"\"\"\n",
        "        Independant GLU block, specific to each step\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, n_glu=2, first=False, shared_layers=None,\n",
        "                 virtual_batch_size=128, momentum=0.02):\n",
        "        super(GLU_Block, self).__init__()\n",
        "        self.first = first\n",
        "        self.shared_layers = shared_layers\n",
        "        self.n_glu = n_glu\n",
        "        self.glu_layers = torch.nn.ModuleList()\n",
        "\n",
        "        params = {\n",
        "            'virtual_batch_size': virtual_batch_size,\n",
        "            'momentum': momentum\n",
        "        }\n",
        "\n",
        "        fc = shared_layers[0] if shared_layers else None\n",
        "        self.glu_layers.append(GLU_Layer(input_dim, output_dim,\n",
        "                                         fc=fc,\n",
        "                                         **params))\n",
        "        for glu_id in range(1, self.n_glu):\n",
        "            fc = shared_layers[glu_id] if shared_layers else None\n",
        "            self.glu_layers.append(GLU_Layer(output_dim, output_dim,\n",
        "                                             fc=fc,\n",
        "                                             **params))\n",
        "\n",
        "    def forward(self, x):\n",
        "        scale = math.sqrt(0.5)\n",
        "        if self.first:  # the first layer of the block has no scale multiplication\n",
        "            x = self.glu_layers[0](x)\n",
        "            layers_left = range(1, self.n_glu)\n",
        "        else:\n",
        "            layers_left = range(self.n_glu)\n",
        "\n",
        "        for glu_id in layers_left:\n",
        "            x = torch.add(x, self.glu_layers[glu_id](x))\n",
        "            x = x*scale\n",
        "        return x\n",
        "\n",
        "\n",
        "class GLU_Layer(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, fc=None,\n",
        "                 virtual_batch_size=128, momentum=0.02):\n",
        "        super(GLU_Layer, self).__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        if fc:\n",
        "            self.fc = fc\n",
        "        else:\n",
        "            self.fc = Linear(input_dim, 2*output_dim, bias=False)\n",
        "        initialize_glu(self.fc, input_dim, 2*output_dim)\n",
        "\n",
        "        self.bn = GBN(2*output_dim, virtual_batch_size=virtual_batch_size,\n",
        "                      momentum=momentum)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.bn(x)\n",
        "        out = torch.mul(x[:, :self.output_dim], torch.sigmoid(x[:, self.output_dim:]))\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0imEJmmfc_U2"
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class TabDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        super().__init__()\n",
        "        self.x = torch.from_numpy(x).type(torch.int64) \n",
        "        self.y = torch.from_numpy(y).type(torch.float32).squeeze() \n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx, :], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "train_dataloader = DataLoader(TabDataset(x_dis_train, y_train_np), batch_size = 32, num_workers=6)\n",
        "test_dataloader = DataLoader(TabDataset(x_dis_test, y_test_np), batch_size = 128, num_workers=6)\n",
        "class TrainingModuleV2(pl.LightningModule):\n",
        "    def __init__(self, x, dim_emb, dim_out, penalty=1e-3, **kwargs):\n",
        "        super().__init__()\n",
        "        self.penalty = penalty\n",
        "        self.embedding = EmbeddingFactory(x, dim_emb)\n",
        "        self.backbone = TabNet(x.shape[1]*dim_emb, dim_out, **kwargs)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.loss = nn.BCELoss()\n",
        "        self.accuracy = Accuracy()\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        return self.backbone(x)\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = self.embedding(x)\n",
        "        x = rearrange(x, 'b n e -> b (n e)')\n",
        "        x, _ = self.backbone(x)\n",
        "        x = self.sigmoid(x.squeeze())\n",
        "        loss = self.loss(x.squeeze(), y.type(torch.float32))\n",
        "        acc = self.accuracy(x.squeeze(), y)\n",
        "        self.log(\"Validation loss\", loss)\n",
        "        self.log(\"Validation acc\", acc)\n",
        "        return loss, acc\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = self.embedding(x)\n",
        "        x = rearrange(x, 'b n e -> b (n e)')\n",
        "        x, m_loss = self.backbone(x)\n",
        "        x = self.sigmoid(x.squeeze())\n",
        "        loss = self.loss(x, y.type(torch.float32)) - self.penalty*m_loss\n",
        "        acc = self.accuracy(x, y)\n",
        "        self.log(\"Training loss\", loss)\n",
        "        self.log(\"Training acc\", acc)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-3)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "training_module = TrainingModuleV2(x_dis, 32, 1, n_steps=2, n_independent=4, n_shared=4,)\n",
        "trainer = pl.Trainer(max_epochs=10, tpu_cores=8, progress_bar_refresh_rate=100, val_check_interval=0.5)\n",
        "trainer.fit(training_module, train_dataloader, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-uyBdKLc_U3"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p80Wv972uJiU"
      },
      "source": [
        "### Embedding with Distance Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLqOqPwduJiW"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import numpy as np\n",
        "class EntityEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, num_level, emdedding_dim, centroid):\n",
        "        super(EntityEmbeddingLayer, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_level, emdedding_dim)\n",
        "        self.centroid = torch.tensor(centroid, dtype=torch.float32).detach_()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    def forward(self, x):\n",
        "        x = x[:, None]\n",
        "        d = 1.0 / ((x - self.centroid).abs() + EPS)\n",
        "        w = self.softmax(d)\n",
        "        v = torch.mm(w, self.embedding.weight)\n",
        "        return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvGjhmHeuJiX"
      },
      "source": [
        "embedding = EntityEmbeddingLayer(20, 4, centroids[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}